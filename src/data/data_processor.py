import re
import json
import random
from pathlib import Path
from tqdm import tqdm
import pandas as pd
from modelscope import AutoTokenizer

MODEL_NAME = "Qwen/Qwen3-0.6B"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# æ•°æ®è·¯å¾„é…ç½®
INPUT_CSV_PATH = "./dataset/training_prompt2.csv"
OUTPUT_JSONL_PATH = "./dataset/train.jsonl"

# å¹¶å‘é…ç½®
MAX_WORKERS = 5  # å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°ï¼Œæ ¹æ®æœåŠ¡å™¨æ€§èƒ½è°ƒæ•´
REQUEST_TIMEOUT = 60  # å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# ç³»ç»Ÿæç¤ºè¯ï¼ˆä¸šåŠ¡è§„åˆ™é›†ä¸­å­˜æ”¾ï¼‰
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ¨¡å¼é“¾æ¥åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®<question>...</question>ä¸­çš„ç”¨æˆ·é—®é¢˜ï¼Œä»<database>...</database>çš„æ•°æ®åº“ä¸­é€‰å–åˆé€‚çš„è¡¨ååŠå¯¹åº”çš„åˆ—åã€‚  
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºç»“æœï¼š
```json
{
  "schema": [
    {
      "table_name": "è¡¨å1",
      "columns": ["åˆ—1", "åˆ—2", "åˆ—3"]
    },
    ...
  ]
}
```
å¦‚æœæ²¡æœ‰åŒ¹é…çš„è¡¨æˆ–åˆ—ï¼Œè¯·è¿”å› {"schema": []}ã€‚ä¸è¦åŒ…å«ä»»ä½•é¢å¤–å­—æ®µã€è§£é‡Šæˆ–æ–‡æœ¬ã€‚
"""


# ========================== å·¥å…·å‡½æ•°ï¼ˆæå–é‡å¤é€»è¾‘ï¼Œå•ä¸€èŒè´£ï¼‰==========================
def _extract_tables_and_columns(standard_str: str) -> tuple[set, set]:
    """
    é€šç”¨å·¥å…·å‡½æ•°ï¼šä»æ ‡å‡†åŒ–å­—ç¬¦ä¸²ä¸­æå–è¡¨åå’Œåˆ—åï¼ˆå°å†™åŒ–ï¼Œé¿å…å¤§å°å†™è¯¯å·®ï¼‰
    :param standard_str: æ ¼å¼å¦‚ "###Tables: è¡¨1,è¡¨2;\n###Columns: è¡¨1.åˆ—1,è¡¨2.åˆ—2;"
    :return: (è¡¨åé›†åˆ, åˆ—åé›†åˆ)
    """
    # æå–è¡¨å
    tables_match = re.search(r'Tables:\s*(.*?);', standard_str)
    tables = set(tables_match.group(1).split(', ')) if (tables_match and tables_match.group(1).strip()) else set()

    # æå–åˆ—å
    cols_match = re.search(r'Columns:\s*(.*?);', standard_str)
    columns = set(col.strip() for col in cols_match.group(1).split(', ')) if (
            cols_match and cols_match.group(1).strip()) else set()

    # ç»Ÿä¸€å°å†™ï¼Œæ¶ˆé™¤å¤§å°å†™å·®å¼‚å½±å“
    return {t.lower() for t in tables}, {c.lower() for c in columns}


def load_input_data(file_path: str) -> pd.DataFrame:
    """åŠ è½½è¾“å…¥CSVæ•°æ®ï¼ˆæ·»åŠ å¼‚å¸¸æ•è·ï¼Œæ˜ç¡®é”™è¯¯æ¥æºï¼‰"""
    try:
        df = pd.read_csv(file_path)
        # æ ¡éªŒå¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = ["question", "database_schema", "target_schema"]
        if not all(col in df.columns for col in required_cols):
            raise ValueError(f"CSVç¼ºå°‘å¿…è¦åˆ—ï¼Œéœ€åŒ…å«ï¼š{required_cols}")
        return df
    except Exception as e:
        raise ValueError(f"æ•°æ®åŠ è½½å¤±è´¥ï¼š{str(e)}")


# ========================== æ•°æ®å¤„ç†æ¨¡å—ï¼ˆçœŸå®æ ‡ç­¾ã€æ¨¡å‹è¾“å…¥æ„å»ºï¼‰==========================
def process_ground_truth(target_schema_raw: str) -> tuple[dict, str]:
    """
    å¤„ç†çœŸå®æ ‡ç­¾schemaï¼šå°†åŸå§‹å­—ç¬¦ä¸²è½¬ä¸ºæ ‡å‡†JSONå’Œæ ‡å‡†åŒ–å­—ç¬¦ä¸²
    :param target_schema_raw: åŸå§‹æ ‡ç­¾å­—ç¬¦ä¸²ï¼ˆå¦‚ "###Tables: singer;\n###Columns: singer.name;"ï¼‰
    :return: (çœŸå®æ ‡ç­¾JSON, çœŸå®æ ‡ç­¾æ ‡å‡†åŒ–å­—ç¬¦ä¸²)
    """
    # æå–è¡¨åå’Œåˆ—å
    truth_tables, truth_cols = _extract_tables_and_columns(target_schema_raw)

    # æ„å»ºçœŸå®æ ‡ç­¾JSONï¼ˆä¸æ¨¡å‹è¾“å‡ºæ ¼å¼å¯¹é½ï¼‰
    ground_truth_json = {"schema": []}
    table_col_map = {table: [] for table in truth_tables}
    for col in truth_cols:
        try:
            table, col_name = col.split(".")
            if table in table_col_map:
                table_col_map[table].append(col_name)
        except ValueError:
            continue  # è·³è¿‡æ ¼å¼é”™è¯¯çš„åˆ—ï¼ˆå¦‚æ— è¡¨åçš„åˆ—ï¼‰

    # å¡«å……JSONç»“æ„
    for table, cols in table_col_map.items():
        ground_truth_json["schema"].append({"table_name": table, "columns": cols})

    # ç”Ÿæˆæ ‡å‡†åŒ–å­—ç¬¦ä¸²ï¼ˆç”¨äºåç»­å¯¹æ¯”ï¼‰
    tables_str = ', '.join(truth_tables)
    cols_str = ', '.join(truth_cols)
    ground_truth_str = f"###Tables: {tables_str};\n###Columns: {cols_str};"

    return ground_truth_json, ground_truth_str


def build_model_prompt(question: str, db_schema: str) -> str:
    """æ„å»ºæ¨¡å‹è¾“å…¥æç¤ºè¯ï¼ˆå°è£…Promptæ ¼å¼ï¼Œä¾¿äºåç»­è°ƒæ•´ï¼‰"""
    return f"<question>{question}</question>\n<database>{db_schema}</database>"


# ========================== ä¸»é€»è¾‘ ==========================
def main():
    df = load_input_data(INPUT_CSV_PATH)
    training_samples = []
    seq_lengths = []

    print("âœ… æ­£åœ¨æ„å»ºè®­ç»ƒæ ·æœ¬...")
    for _, row in tqdm(df.iterrows(), total=len(df), desc="æ„å»ºæ ·æœ¬"):
        # 1. å¤„ç† ground truth
        gt_json, gt_str = process_ground_truth(row["target_schema"])

        # 2. æ„å»ºå¯¹è¯æ¶ˆæ¯ï¼ˆä¸å« assistantï¼Œç”¨äºè®­ç»ƒè¾“å…¥ï¼‰
        user_content = build_model_prompt(row["question"], row["database_schema"])
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_content}
        ]

        # 3. è®¡ç®—å®Œæ•´å¯¹è¯ï¼ˆå« assistant å›å¤ï¼‰çš„ token é•¿åº¦ï¼Œç”¨äºåˆ†æ
        full_messages = messages + [{"role": "assistant", "content": json.dumps(gt_json, ensure_ascii=False, indent=None)}]
        full_text = tokenizer.apply_chat_template(
            full_messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=True
        )
        input_ids = tokenizer([full_text], return_tensors="pt")["input_ids"]
        seq_len = input_ids.shape[1]
        seq_lengths.append(seq_len)

        # 4. ä¿å­˜è®­ç»ƒæ ·æœ¬ï¼ˆä»…å« prompt + ground truthï¼Œä¸å« full å¯¹è¯ï¼‰
        training_samples.append({
            "prompt": messages,
            "question": row["question"],
            "ground_truth": gt_json,
            "ground_truth_standard_str": gt_str,
            "seq_length": seq_len
        })

    # éšæœºæ‰“ä¹±
    random.seed(42)
    random.shuffle(training_samples)

    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    max_len = max(seq_lengths) if seq_lengths else 0
    print(f"\nâœ… æœ€å¤§åºåˆ—é•¿åº¦: {max_len}")  # 2258
    print(f"\nâœ… æ€»æ ·æœ¬æ•°: {len(training_samples)}") # 8529

    # ä¿å­˜ä¸º JSONL
    output_path = Path(OUTPUT_JSONL_PATH)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ’¾ æ­£åœ¨å†™å…¥ JSONL æ–‡ä»¶: {output_path}")
    with open(output_path, "w", encoding="utf-8") as f:
        for sample in tqdm(training_samples, desc="å†™å…¥æ–‡ä»¶"):
            # ç§»é™¤ seq_lengthï¼ˆè‹¥ä»…ç”¨äºåˆ†æå¯ä¿ç•™ï¼›æ­¤å¤„æŒ‰éœ€ä¿ç•™ï¼‰
            f.write(json.dumps(sample, ensure_ascii=False) + "\n")

    print("ğŸ‰ æ•°æ®å‡†å¤‡å®Œæˆï¼")


if __name__ == "__main__":
    main()
