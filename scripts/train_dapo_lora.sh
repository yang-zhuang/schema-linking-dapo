#!/bin/bash
# ==============================
# GRPO è®­ç»ƒè„šæœ¬ - Qwen3-0.6B æ¨¡å‹
# ç”¨äºSQLç”Ÿæˆä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ
# ==============================

# ------------------------------
# æ•°æ®é…ç½®
# ------------------------------
# è®­ç»ƒæ•°æ®é›†è·¯å¾„ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
DATASET_PATH="data/train.jsonl"
# è¾“å…¥æç¤ºçš„æœ€å¤§é•¿åº¦
MAX_PROMPT_LENGTH=2048
# ç”Ÿæˆç»“æœçš„æœ€å¤§é•¿åº¦
MAX_COMPLETION_LENGTH=1024

# ------------------------------
# æ¨¡å‹é…ç½®
# ------------------------------
# åŸºç¡€æ¨¡å‹è·¯å¾„ (æ³¨æ„ï¼šWindowsè·¯å¾„ä½¿ç”¨æ­£æ–œæ æˆ–åŒåæ–œæ )
#MODEL_PATH="/mnt/d/modelscope/Qwen3-0.6B-GPTQ-Int8"
MODEL_PATH="/root/autodl-tmp/modelscope/Qwen3-0.6B"
# è®¡ç®—ç²¾åº¦ (bfloat16/float16/float32)
DTYPE="bfloat16"
# æ˜¯å¦ä½¿ç”¨PEFTå‚æ•°é«˜æ•ˆå¾®è°ƒ
USE_PEFT="--use_peft"

#############
vllm_gpu_memory_utilization=0.5
vllm_mode='colocate'
use_vllm="--use_vllm"
logging_steps=10
load_in_4bit="--load_in_4bit"
use_liger_kernel="--use_liger_kernel"
num_train_epochs=1
save_total_limit=2
save_steps=50
save_strategy="steps"
########

# ------------------------------
# è®­ç»ƒé…ç½®
# ------------------------------
# æ€»è®­ç»ƒè¿­ä»£æ¬¡æ•°
NUM_ITERATIONS=2
# æ¯æ¬¡è¿­ä»£ç”Ÿæˆçš„æ ·æœ¬æ•°
NUM_GENERATIONS=4
# æ¯æ¬¡ç”Ÿæˆçš„ä¼˜åŒ–æ­¥æ•°
STEPS_PER_GENERATION=4
# æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
GRADIENT_ACCUMULATION_STEPS=2
# æ¯è®¾å¤‡æ‰¹å¤§å°
BATCH_SIZE=2
# æŸå¤±å‡½æ•°ç±»å‹
LOSS_TYPE="dapo"
# é‡è¦æ€§é‡‡æ ·çº§åˆ«
SAMPLING_LEVEL="token"

# ------------------------------
# ä¼˜åŒ–å™¨é…ç½®
# ------------------------------
# å­¦ä¹ ç‡
LEARNING_RATE=1e-5
# KLæ•£åº¦æ§åˆ¶å‚æ•° (ä½)
EPSILON=0.2
# KLæ•£åº¦æ§åˆ¶å‚æ•° (é«˜)
EPSILON_HIGH=0.28
# ä¼˜åŠ¿å‡½æ•°å¹³æ»‘ç³»æ•°
BETA=0.0

# ------------------------------
# å¥–åŠ±å‡½æ•°é…ç½®
# ------------------------------
# æ³¨æ„ï¼šæ¯ä¸ªå¥–åŠ±å‡½æ•°å•ç‹¬é…ç½®ï¼Œæé«˜å¯ç»´æŠ¤æ€§
#"rewards.schema_selection_reward.table_reward"       # è¡¨é€‰æ‹©å¥–åŠ±
#"rewards.schema_selection_reward.table_penalty"      # è¡¨é€‰æ‹©æƒ©ç½š
#"rewards.schema_selection_reward.column_reward"      # åˆ—é€‰æ‹©å¥–åŠ±
#"rewards.schema_selection_reward.column_penalty"     # åˆ—é€‰æ‹©æƒ©ç½š
#"rewards.format_rewards.think_tag_penalty"           # thinkæ ‡ç­¾æ ¼å¼æƒ©ç½š
#"rewards.format_rewards.valid_json_reward"           # æœ‰æ•ˆJSONæ ¼å¼å¥–åŠ±
#"rewards.other_rewards.get_soft_overlong_punishment_medium"  # è¿‡é•¿è¾“å‡ºæƒ©ç½š

# ------------------------------
# è¾“å‡ºé…ç½®
# ------------------------------
# æ¨¡å‹è¾“å‡ºç›®å½•ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰
OUTPUT_DIR="outputs/dapo-Qwen3-0.6B"
# æ˜¯å¦è®°å½•ç”Ÿæˆçš„å®Œæ•´å†…å®¹
LOG_COMPLETIONS="--log_completions"

# ==============================
# è·¯å¾„è°ƒè¯•ä¿¡æ¯
# ==============================
echo "ğŸ”§ è°ƒè¯•ä¿¡æ¯:"
echo "ğŸ“ è„šæœ¬ç›®å½•: $SCRIPT_DIR"
echo "ğŸ“ é¡¹ç›®æ ¹ç›®å½•: $PROJECT_ROOT"
echo "ğŸ“„ æ•°æ®æ–‡ä»¶è·¯å¾„: $PROJECT_ROOT/$DATASET_PATH"
echo "ğŸ“„ æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨: $(test -f "$PROJECT_ROOT/$DATASET_PATH" && echo "âœ… å­˜åœ¨" || echo "âŒ ä¸å­˜åœ¨")"
echo "ğŸ“¤ è¾“å‡ºç›®å½•: $PROJECT_ROOT/$OUTPUT_DIR"
echo ""

# ==============================
# æ‰§è¡Œè®­ç»ƒå‘½ä»¤
# ==============================
#num_train_epochs=1
 # save_total_limit=2
 # save_steps=10
 # save_strategy="steps"

cd "$PROJECT_ROOT" && PYTHONPATH="$PROJECT_ROOT:$PYTHONPATH" python src/training/grpo.py \
    --dataset_name "$DATASET_PATH" \
    --model_name_or_path "$MODEL_PATH" \
    --output_dir "$OUTPUT_DIR" \
    --max_prompt_length "$MAX_PROMPT_LENGTH" \
    --max_completion_length "$MAX_COMPLETION_LENGTH" \
    --vllm_mode "$vllm_mode" \
    --vllm_gpu_memory_utilization "$vllm_gpu_memory_utilization" \
    --num_train_epochs "$num_train_epochs" \
    --save_total_limit "$save_total_limit" \
    --save_steps "$save_steps" \
    --save_strategy "$save_strategy" \
    --logging_steps "$logging_steps" \
    $load_in_4bit \
    $use_vllm \
    $USE_PEFT \
    $LOG_COMPLETIONS \
    --learning_rate "$LEARNING_RATE" \
    --dtype "$DTYPE" \
    --per_device_train_batch_size "$BATCH_SIZE" \
    --num_generations "$NUM_GENERATIONS" \
    --gradient_accumulation_steps "$GRADIENT_ACCUMULATION_STEPS" \
    --steps_per_generation "$STEPS_PER_GENERATION" \
    --epsilon "$EPSILON" \
    --epsilon_high "$EPSILON_HIGH" \
    --beta "$BETA" \
    --reward_funcs "src.rewards.schema_rewards.table_reward" "src.rewards.schema_rewards.table_penalty" "src.rewards.schema_rewards.column_reward" "src.rewards.schema_rewards.column_penalty" "src.rewards.format_rewards.think_tag_penalty" "src.rewards.format_rewards.valid_json_reward" "src.rewards.base_rewards.get_soft_overlong_punishment_medium" \
    --num_iterations "$NUM_ITERATIONS" \
    --loss_type "$LOSS_TYPE" \
    --importance_sampling_level "$SAMPLING_LEVEL" \
    --report_to tensorboard